{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvfm4rmPFRwf"
      },
      "source": [
        "# Установка и импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C0GFmz3FMoG",
        "outputId": "a1bf32f2-d12b-45bb-8a01-8ab9f5ca65f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.7)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.2.9)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.3)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.6.9)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (9.1.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.14.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Requirement already satisfied: groq<1.0.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.37.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.8 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (1.2.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.15.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain-groq) (0.6.9)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain-groq) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain-groq) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain-groq) (9.1.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain-groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.8->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain-groq) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain-groq) (2.32.4)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain-groq) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain-groq) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain-groq) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain-groq) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install langgraph\n",
        "!pip3 install -U langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEmvYCQzFXH-"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Annotated, Literal\n",
        "from langgraph.graph.message import add_messages\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_groq import ChatGroq\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSSinVlbFZvM"
      },
      "source": [
        "# Инициализация llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFKGrkmKFhNd"
      },
      "outputs": [],
      "source": [
        "llm = ChatGroq(\n",
        "    model='llama-3.3-70b-versatile'\n",
        "    , temperature=0.1\n",
        "    , api_key=''\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ww4hzi3F1y1"
      },
      "source": [
        "# Логика графа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaGwfH2lFygh"
      },
      "outputs": [],
      "source": [
        "class UserState(TypedDict):\n",
        "    # Инициализируется сразу\n",
        "    messages: Annotated[list, add_messages] # - сообщение, с которой пришел пользователь\n",
        "    data: pd.DataFrame                      # - датасет\n",
        "\n",
        "    target: str                             # - переменной по которой будем делать стат тесты\n",
        "    grouping_variable: str                  # - все переменные которые пойдут в логику исследования\n",
        "    grouping_values: List[str]              # - разделюящие группирующую переменную на выборки\n",
        "    satisfied: str                          # - состояние выбронных переменных\n",
        "    is_normal_target: bool                  # - нормальное ли распределение таргета\n",
        "\n",
        "    samples: List[str]                      # - группы для стат теста\n",
        "\n",
        "    target_dtype: str                       # - тип целевой переменной\n",
        "    grouping_variable_dtype: str            # - тип группирующей переменной\n",
        "\n",
        "    method_of_var_analisis: str             # - метод дисперсионного анализа**\n",
        "\n",
        "    artefacts: List[str]                    # - все что будет переданно в llm для вывода сводки\n",
        "\n",
        "    summary: str                            # - сводка исследования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AchGz4SyrLAU"
      },
      "source": [
        "## Определение переменных исследования"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fUDbtA4SU95"
      },
      "outputs": [],
      "source": [
        "class TargetExtraction(BaseModel):\n",
        "    target: str = Field(description='Название зависимой переменной. По которой будет производиться стат тест. Тип может быть любой.')\n",
        "    grouping_variable: str = Field(description='Группирующая переменная. Если подразумеватеся проверка нескольких групп, рекомендуется использовать одну переменную. Тип может быть любой.')\n",
        "    grouping_values: List[str] = Field(description='Значения, разделюящие группирующую переменную на выборки (в том числе и какие-то значение числовой переменной, НО передавать строго списком, из str значений). Также тут можем быть СПИСОК с одним значением \"ALL\", если к примеру, необходимо провести анализ взаимосвязи двх количественных переменных.')\n",
        "\n",
        "def find_variables(state: UserState) -> UserState:\n",
        "    data = state['data']\n",
        "    all_counts = {}\n",
        "    for col in data.columns:\n",
        "        counts = data[col].value_counts().head(10).to_dict()\n",
        "        if len(data[col].unique()) > 10:\n",
        "            counts['...'] = f'and {len(data[col].unique()) - 10} more'\n",
        "\n",
        "        all_counts[col] = counts\n",
        "    sys_prompt = f'Ты помощник аналитика. По сообщению пользователя, тебе необходимо определить значения зависимой переменной и группирующей. Исходи из того, что хочет пользователь.'\\\n",
        "                f'Вот данные, пользователя. dtypes: {data.dtypes.to_string()}\\nvalues counts: {json.dumps(all_counts, ensure_ascii=False, indent=2)}'\n",
        "\n",
        "    messages = state['messages']\n",
        "    instruction = HumanMessage(content='Внимательно изучи последние системные сообщения об ошибках и исправь свой выбор переменных согласно им. Не повторяй предыдущие ошибки.')\n",
        "    structured_llm = llm.with_structured_output(TargetExtraction)\n",
        "\n",
        "    result = structured_llm.invoke([sys_prompt] + messages + [instruction])\n",
        "\n",
        "    return {\n",
        "        'target': result.target\n",
        "        , 'grouping_variable': result.grouping_variable\n",
        "        , 'grouping_values': result.grouping_values\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0o-yYqSXHqy"
      },
      "outputs": [],
      "source": [
        "class CorrectingVariables(BaseModel):\n",
        "    answer: Literal['YES', 'NO'] = Field(description='Выбор удовлетворил пользователя? Толко YES или NO')\n",
        "    correction_message: str = Field(description='Опиши переменные которые пользователь хочет поменять и на что, а какие хочет оставить. Если пользователя все устроило, оставь пробел.')\n",
        "\n",
        "def is_correct_variables(state: UserState) -> UserState:\n",
        "    print(f'Выбран таргет: {state['target']}, групповая переменная: {state['grouping_variable']}, ее значения: {' '.join(state['grouping_values'])}')\n",
        "    response = input('Вас удовлетворяет выбор? ')\n",
        "    structured_llm = llm.with_structured_output(CorrectingVariables)\n",
        "    analysis = structured_llm.invoke(\n",
        "        f'Пользователь ответил на выбор переменных. \\n'\n",
        "        f'Выбрано: target={state['target']}, group={state['grouping_variable']}. \\n'\n",
        "        f'Ответ пользователя: {response}'\n",
        "    )\n",
        "    new_messages = state.get('messages', [])\n",
        "    if analysis.answer == 'NO':\n",
        "        feedback_msg = SystemMessage(content=f\"ОШИБКА: Пользователь недоволен. Исправь выбор. {analysis.correction_message}\")\n",
        "        new_messages.append(feedback_msg)\n",
        "    return {\n",
        "        'satisfied': analysis.answer,\n",
        "        'messages': new_messages\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjveL9QyrPsg"
      },
      "source": [
        "## другие функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-bf4mVD3PXJ"
      },
      "outputs": [],
      "source": [
        "def process_dtypes_strictly(state: UserState) -> UserState:\n",
        "    data = state['data'].copy()\n",
        "    t_col = state['target']\n",
        "    g_col = state['grouping_variable']\n",
        "\n",
        "    old_dtypes = data[[t_col, g_col]].dtypes\n",
        "    selected_types = {}\n",
        "\n",
        "    for col in [t_col, g_col]:\n",
        "        unique_vals = set(data[col].dropna().unique())\n",
        "\n",
        "        if unique_vals == {0, 1} or unique_vals == {0.0, 1.0}:\n",
        "            new_type = 'string'\n",
        "        else:\n",
        "            new_type = str(data[col].dtype)\n",
        "\n",
        "        selected_types[col] = new_type\n",
        "\n",
        "        try:\n",
        "            data[col] = data[col].astype(new_type)\n",
        "        except Exception as e:\n",
        "            print(f'Ошибка конвертации {col}: {e}')\n",
        "\n",
        "    new_dtypes = data[[t_col, g_col]].dtypes\n",
        "\n",
        "    if old_dtypes.to_string() != new_dtypes.to_string():\n",
        "        artefact = (\n",
        "            f'Изменены типы данных (бинарка 0/1 -> string):\\n'\n",
        "            f'БЫЛО:\\n{old_dtypes.to_string()}\\n'\n",
        "            f'СТАЛО:\\n{new_dtypes.to_string()}'\n",
        "        )\n",
        "    else:\n",
        "        artefact = 'Типы данных без изменений.'\n",
        "\n",
        "    return {\n",
        "        'data': data,\n",
        "        'target_dtype': selected_types[t_col],\n",
        "        'grouping_variable_dtype': selected_types[g_col],\n",
        "        'artefacts': state.get('artefacts', []) + [artefact]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwXzJMf6kExH"
      },
      "outputs": [],
      "source": [
        "def cleaning_target(state: UserState) -> UserState:\n",
        "    data = state['data'].copy()\n",
        "    target = state['target']\n",
        "    grouping_variable = state['grouping_variable']\n",
        "\n",
        "    lower_bound = None\n",
        "    upper_bound = None\n",
        "\n",
        "    if data[target].dtype in ['float64', 'int64']:\n",
        "        q1 = data[target].quantile(0.25)\n",
        "        q3 = data[target].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "        data = data[(data[target] >= lower_bound) & (data[target] <= upper_bound)]\n",
        "\n",
        "    if lower_bound is not None:\n",
        "        count_removed = len(state['data']) - len(data)\n",
        "        artefact = f'Были очищены выбросы: {count_removed}. Границы: {lower_bound:.2f} - {upper_bound:.2f}'\n",
        "    else:\n",
        "        artefact = f'Переменная {target} не является числовой, очистка выбросов пропущена.'\n",
        "\n",
        "    if data[grouping_variable].dtype in ['float64', 'int64']:\n",
        "        q1 = data[grouping_variable].quantile(0.25)\n",
        "        q3 = data[grouping_variable].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "        data = data[(data[grouping_variable] >= lower_bound) & (data[grouping_variable] <= upper_bound)]\n",
        "\n",
        "    if lower_bound is not None:\n",
        "        count_removed = len(state['data']) - len(data)\n",
        "        artefact += f'Были очищены выбросы: {count_removed}. Границы: {lower_bound:.2f} - {upper_bound:.2f}'\n",
        "    else:\n",
        "        artefact += f'Переменная {grouping_variable} не является числовой, очистка выбросов пропущена.'\n",
        "\n",
        "    return {\n",
        "        'data': data,\n",
        "        'artefacts': state.get('artefacts', []) + [artefact]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azZAUgmiZlt3"
      },
      "source": [
        "# Узел перехода к стат тестам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrUy4lZNZlKv"
      },
      "outputs": [],
      "source": [
        "def identifying_variables(state: UserState) -> UserState:\n",
        "    # target & grouping_variable\n",
        "    data = state['data']\n",
        "    groups = state['grouping_values']\n",
        "\n",
        "    target = state['target']\n",
        "    grouping_variable = state['grouping_variable']\n",
        "\n",
        "    len_of_groups = len(groups)\n",
        "    if len_of_groups == 1 or len_of_groups == 2:\n",
        "        if is_numeric_dtype(data[target]) and is_numeric_dtype(data[grouping_variable]):\n",
        "            if groups[0] == 'ALL':\n",
        "                return 'ALL_NUMERIC' # две количественные\n",
        "            else:\n",
        "                return 'ONE_NUMERIC_AND_ONE_NOMINATIV' # одна номинативная друга количественная\n",
        "        elif not is_numeric_dtype(data[target]) and not is_numeric_dtype(data[grouping_variable]):\n",
        "            return 'ALL_NOMINATIV' # обе номинативные\n",
        "        elif is_numeric_dtype(data[target]) and not is_numeric_dtype(data[grouping_variable]):\n",
        "            return 'ONE_NUMERIC_AND_ONE_NOMINATIV' # одна номинативная друга количественная\n",
        "    else:\n",
        "        if is_numeric_dtype(data[target]):\n",
        "            return 'MANY_GROUPS_AND_NUMERIC_TARGET' # таргет количественный\n",
        "        elif not is_numeric_dtype(data[target]):\n",
        "            return 'MANY_GROUPS_AND_NOMINATIV_TARGET' # таргет номинативный"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB1nSxWRevKn"
      },
      "source": [
        "# Обе переменные количественные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDA6Q-Fze2qS"
      },
      "outputs": [],
      "source": [
        "# identifying_variables -> ALL_NUMERIC\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def make_lin_reg(state: UserState) -> UserState:\n",
        "    data = state['data']\n",
        "    x = sm.add_constant(data[state['grouping_variable']])\n",
        "    y = data[state['target']]\n",
        "    model = sm.OLS(y, x).fit()\n",
        "    artefact = f'По скольку в выбранных переменных обе переменные количественные, построим линейную регрессию. Сводка (summary):\\n{str(model.summary())}'\n",
        "    print('Уже строю линейную регрессию!')\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}\n",
        "\n",
        "def make_corrs(state: UserState) -> UserState:\n",
        "    data = state['data']\n",
        "    pr_stat, _ = pearsonr(data[state['target']], data[state['grouping_variable']])\n",
        "    sp_stat, _ = spearmanr(data[state['target']], data[state['grouping_variable']])\n",
        "    artefact = f'Построим корреляции между признаками, pearson: {pr_stat}, spearman: {sp_stat}'\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bhAroivp_JV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSE1qbi9qA42"
      },
      "source": [
        "# Обе переменнные номинативные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfWv1BNZqEVJ"
      },
      "outputs": [],
      "source": [
        "# identifying_variables -> ALL_NOMINATIV\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats import fisher_exact\n",
        "\n",
        "def make_chi2(state: UserState) -> UserState:\n",
        "    data = state['data']\n",
        "    target = state['target']\n",
        "    grouping_variable = state['grouping_variable']\n",
        "    contingency_table = pd.crosstab(data[target], data[grouping_variable])\n",
        "    res = chi2_contingency(contingency_table)\n",
        "    print('Уже проверию Хи квадратом Пирсона!')\n",
        "    artefact = f'Был проведен анализ таблицы сопряженности, вот таблица: {contingency_table}, вот результат (Хи-квадрат): {res}'\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}\n",
        "\n",
        "def make_fisher_exact(state: UserState) -> UserState:\n",
        "    data = state['data']\n",
        "    target = state['target']\n",
        "    grouping_variable = state['grouping_variable']\n",
        "    contingency_table = pd.crosstab(data[target], data[grouping_variable])\n",
        "    res = fisher_exact(contingency_table)\n",
        "    artefact = f'Также был проведен точный критерий фишера: {res}'\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ni9XOUitVV3"
      },
      "source": [
        "# Одна переменная количественная вторая номинативная"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAzwlzyPRJRU",
        "outputId": "fbcc808d-5911-47d8-fce1-9a55f3e5d8b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        }
      ],
      "source": [
        "# identifying_variables -> ONE_NUMERIC_AND_ONE_NOMINATIV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import io\n",
        "import PIL.Image\n",
        "from scipy.stats import probplot\n",
        "from scipy.stats import shapiro\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key='')\n",
        "\n",
        "class IsNormal(BaseModel):\n",
        "    is_norm: bool = Field(description='Ответ True или False, распределение признака схоже с нормальным?')\n",
        "\n",
        "vision_llm = genai.GenerativeModel(\n",
        "    model_name='gemini-flash-latest',\n",
        "    generation_config={\n",
        "        'response_mime_type': 'application/json',\n",
        "        'response_schema': IsNormal\n",
        "    }\n",
        ")\n",
        "\n",
        "def is_normal_distribution(state: UserState) -> UserState:\n",
        "    data = state['data']\n",
        "    target = state['target']\n",
        "    grouping_variable = state['grouping_variable']\n",
        "    grouping_values = state['grouping_values']\n",
        "\n",
        "    if not is_numeric_dtype(data[grouping_variable]):\n",
        "        sample_one = data[data[grouping_variable] == grouping_values[0]][target].dropna().to_numpy()\n",
        "        sample_two = data[data[grouping_variable] == grouping_values[1]][target].dropna().to_numpy()\n",
        "    else:\n",
        "        sample_one = data[data[grouping_variable] <= float(grouping_values[0])][target].dropna().to_numpy()\n",
        "        sample_two = data[data[grouping_variable] > float(grouping_values[0])][target].dropna().to_numpy()\n",
        "\n",
        "    samples = [sample_one, sample_two]\n",
        "    group_names = [f\"Группа {grouping_values[0]}\", f\"Группа {grouping_values[1]}\"]\n",
        "\n",
        "    results_is_normal = []\n",
        "    total_artefacts = state.get('artefacts', [])\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        n_obs = len(sample)\n",
        "        stat, pval = shapiro(sample)\n",
        "\n",
        "        if pval > 0.05:\n",
        "            artefact = f'В группе {group_names[i]} признак {target} распределен нормально (Шапиро-Уилк, pval={pval:.4f})'\n",
        "            total_artefacts.append(artefact)\n",
        "            results_is_normal.append(True)\n",
        "            continue\n",
        "\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
        "        probplot(sample, dist='norm', plot=axes[0])\n",
        "        axes[0].set_title(f'QQ-plot: {group_names[i]}')\n",
        "        sns.histplot(sample, bins=25, ax=axes[1], kde=True)\n",
        "        axes[1].set_title(f'Распределение: {group_names[i]}')\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format='png')\n",
        "        buf.seek(0)\n",
        "        img = PIL.Image.open(buf)\n",
        "\n",
        "        prompt = (f'Распределение в группе \"{group_names[i]}\" схожее с нормальным? '\n",
        "                  f'Размер выборки N={n_obs}. Тест Шапиро-Уилка: stat={stat:.4f}, pval={pval:.4f}. '\n",
        "                  f'Учти ЦПМ: при больших N тест чувствителен. Если на графиках распределение близко к нормальному, '\n",
        "                  f'можно пренебречь тестом.')\n",
        "\n",
        "        response = vision_llm.generate_content([prompt, img])\n",
        "        plt.close(fig)\n",
        "\n",
        "        answer_is_normal = IsNormal.model_validate_json(response.text).is_norm\n",
        "        results_is_normal.append(answer_is_normal)\n",
        "\n",
        "        status_text = 'Нормально' if answer_is_normal else 'Ненормально'\n",
        "        artefact = f'Визуальный анализ {group_names[i]}: {status_text}. (Шапиро-Уилк pval={pval:.4f})'\n",
        "        total_artefacts.append(artefact)\n",
        "\n",
        "    final_is_normal = all(results_is_normal)\n",
        "\n",
        "    return {\n",
        "        'is_normal_target': final_is_normal,\n",
        "        'artefacts': total_artefacts\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWI07HrgbV9t"
      },
      "outputs": [],
      "source": [
        "def make_samples(state: UserState) -> UserState:\n",
        "    data = state['data']\n",
        "    target = state['target']\n",
        "    grouping_variable = state['grouping_variable']\n",
        "    grouping_values = state['grouping_values']\n",
        "    if not is_numeric_dtype(data[grouping_variable]):\n",
        "        sample_one = data[data[grouping_variable] == grouping_values[0]]\n",
        "        sample_two = data[data[grouping_variable] == grouping_values[1]]\n",
        "    else:\n",
        "        sample_one = data[data[grouping_variable] <= float(grouping_values[0])]\n",
        "        sample_two = data[data[grouping_variable] > float(grouping_values[1])]\n",
        "    samples = [sample_one[target].to_numpy(), sample_two[target].to_numpy()]\n",
        "    return {'samples': samples}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ciAQYQddjwY"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import levene\n",
        "def choose_stat_test(state: UserState) -> UserState:\n",
        "    sampl1, sampl2 = state['samples']\n",
        "    is_normal_target = state['is_normal_target']\n",
        "\n",
        "    if is_normal_target:\n",
        "        stat, pval = levene(sampl1, sampl2)\n",
        "        if pval > 0.05:\n",
        "            return 'T_TEST'\n",
        "        else:\n",
        "            return 'WELCH_T_TEST'\n",
        "    else:\n",
        "        return 'MANAYITHUI' # охх обожаю, но остерегаюсь"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpo-_Vavf-_d"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "def make_ttest(state: UserState) -> UserState:\n",
        "    sampl1, sampl2 = state['samples']\n",
        "\n",
        "    stat, pval = ttest_ind(sampl1, sampl2)\n",
        "\n",
        "    artefact = f'Так как признак распределен нормально, и выборочные дисперсии равны (тест Левенне), мы проводим t-test, его результат: stat-{stat}, pval-{pval}.'\n",
        "\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeQ4l0hTgqTO"
      },
      "outputs": [],
      "source": [
        "def make_welch_ttest(state: UserState) -> UserState:\n",
        "    sampl1, sampl2 = state['samples']\n",
        "\n",
        "    stat, pval = ttest_ind(sampl1, sampl2, equal_var=False)\n",
        "\n",
        "    artefact = f'Так как признак распределен нормально, и выборочные дисперсии не равны (тест Левенне), мы проводим t-test welch-а, его результат: stat-{stat}, pval-{pval}.'\n",
        "\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjS2EUaZJo01"
      },
      "outputs": [],
      "source": [
        "def make_manayithui(state: UserState):\n",
        "    print('Применяю стат тест!')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIVVNdLoHQzm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def choose_method_to_make_manayithui(state: UserState) -> UserState:\n",
        "    sampl1, sampl2 = state['samples']\n",
        "    bakets1 = np.array_split(sampl1, 500)\n",
        "    bakets2 = np.array_split(sampl2, 500)\n",
        "\n",
        "    if len(bakets1[0]) < 10 or len(bakets2[0]) < 10:\n",
        "        return 'STANDART_MANAYITHUI'\n",
        "    else:\n",
        "        return 'BAKET_MANAYITHUI'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcE2Pw4ag7Ah"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "def make_standart_manayithui(state: UserState):\n",
        "    sampl1, sampl2 = state['samples']\n",
        "    stat, pval = mannwhitneyu(sampl1, sampl2)\n",
        "\n",
        "    artefact = f'Так как признак распределен не нормально (Визуальный анализ и тест Шапиро Уилка) => используем стандартный стат тест Манна Уитни: stat-{stat}, pval-{pval}'\n",
        "\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G32YrrthKfc0"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "def make_baket_manayithui(state: UserState):\n",
        "    sampl1, sampl2 = state['samples']\n",
        "\n",
        "    s1_shuffled = np.random.permutation(sampl1)\n",
        "    s2_shuffled = np.random.permutation(sampl2)\n",
        "\n",
        "    bakets1 = np.array_split(s1_shuffled, 500)\n",
        "    bakets2 = np.array_split(s2_shuffled, 500)\n",
        "\n",
        "    baket1 = np.mean(bakets1, axis=1)\n",
        "    baket2 = np.mean(bakets2, axis=1)\n",
        "\n",
        "    stat, pval = mannwhitneyu(baket1, baket2)\n",
        "\n",
        "    artefact = f'Так как признак распределен не нормально (Визуальный анализ и тест Шапиро Уилка). А также данных достаточно для бакетизации.'\\\n",
        "                f'Мы случайным образом будем создавать бакеты по >10 наблюдения в каждом, считать среднее и после проводить тест Манна Уитни.'\\\n",
        "                f'Что даст устойчивости и мощности тесту. \\nstat-{stat}, pval-{pval}'\n",
        "\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n14-ojy1MaNw"
      },
      "source": [
        "# Много групп и зависимая количественная"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaRDYBWOTFgH"
      },
      "outputs": [],
      "source": [
        "def is_normal_distribution_for_var(state: UserState) -> UserState:\n",
        "    data = state['data']\n",
        "    target = state['target']\n",
        "    grouping_variable = state['grouping_variable']\n",
        "\n",
        "    groups = data[grouping_variable].unique()\n",
        "\n",
        "    total_artefacts = state.get('artefacts', [])\n",
        "    results_is_normal = []\n",
        "\n",
        "    for group_label in groups:\n",
        "        sample = data[data[grouping_variable] == group_label][target].dropna().to_numpy()\n",
        "\n",
        "        if len(sample) < 3:\n",
        "            results_is_normal.append(True)\n",
        "            continue\n",
        "\n",
        "        stat, pval = shapiro(sample)\n",
        "\n",
        "        if pval > 0.05:\n",
        "            artefact = f'Группа {group_label}: признак {target} распределен нормально (Шапиро-Уилк, pval={pval:.4f})'\n",
        "            total_artefacts.append(artefact)\n",
        "            results_is_normal.append(True)\n",
        "            continue\n",
        "\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
        "\n",
        "        probplot(sample, dist='norm', plot=axes[0])\n",
        "        axes[0].set_title(f'QQ-plot: Группа {group_label}')\n",
        "\n",
        "        sns.histplot(sample, bins=20, ax=axes[1], kde=True)\n",
        "        axes[1].set_title(f'Распределение: Группа {group_label}')\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format='png')\n",
        "        buf.seek(0)\n",
        "        img = PIL.Image.open(buf)\n",
        "\n",
        "        prompt = (f'Распределение группы \"{group_label}\" (признак {target}) схожее с нормальным? '\n",
        "                  f'Размер выборки N={len(sample)}. Тест Шапиро-Уилка: stat={stat:.4f}, pval={pval:.4f}. '\n",
        "                  f'Учти ЦПМ: при больших N тест чувствителен. Если на графиках распределение визуально нормальное, '\n",
        "                  f'можешь подтвердить нормальность.')\n",
        "\n",
        "        response = vision_llm.generate_content([prompt, img])\n",
        "        plt.close(fig)\n",
        "\n",
        "        answer_is_normal = IsNormal.model_validate_json(response.text).is_norm\n",
        "        results_is_normal.append(answer_is_normal)\n",
        "\n",
        "        status_text = 'Нормально' if answer_is_normal else 'Ненормально'\n",
        "        artefact = f'Группа {group_label}: визуальный анализ показал \"{status_text}\". (Шапиро-Уилк pval={pval:.4f})'\n",
        "        total_artefacts.append(artefact)\n",
        "\n",
        "    final_is_normal = all(results_is_normal)\n",
        "\n",
        "    return {\n",
        "        'is_normal_target': final_is_normal,\n",
        "        'artefacts': total_artefacts\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcNKom7yOmxd"
      },
      "outputs": [],
      "source": [
        "def choose_stat(state: UserState) -> UserState:\n",
        "    # is_normal_distribution -> data\n",
        "    if state['is_normal_target']:\n",
        "        data = state['data']\n",
        "        target = state['target']\n",
        "        grouping_variable = state['grouping_variable']\n",
        "\n",
        "        if is_numeric_dtype(data[grouping_variable]):\n",
        "            grouping_values = state['grouping_values']\n",
        "            values = data['target']\n",
        "            indices = np.searchsorted(np.sort(grouping_values), values)\n",
        "            samples = [values[indices == i] for i in range(len(grouping_values) + 1)]\n",
        "        else:\n",
        "            samples = [group[target].values for name, group in data.groupby(grouping_variable)]\n",
        "\n",
        "        samples = [i for i in samples if len(i) > 0]\n",
        "\n",
        "        stat, pval = levene(*samples)\n",
        "\n",
        "        if pval > 0.05:\n",
        "            return {'method_of_var_analisis': 'ANOVA'}\n",
        "    return {'method_of_var_analisis': 'KRUSKAL'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKUxv6CYMj3N"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import f_oneway\n",
        "from scipy.stats import kruskal\n",
        "def make_var_analisis(state: UserState) -> UserState:\n",
        "    data = state['data']\n",
        "    target = state['target']\n",
        "    grouping_variable = state['grouping_variable']\n",
        "\n",
        "    if is_numeric_dtype(data[grouping_variable]):\n",
        "        grouping_values = state['grouping_values']\n",
        "        values = data['target']\n",
        "        indices = np.searchsorted(np.sort(grouping_values), values)\n",
        "        samples = [values[indices == i] for i in range(len(grouping_values) + 1)]\n",
        "    else:\n",
        "        samples = [group[target].values for name, group in data.groupby(grouping_variable)]\n",
        "\n",
        "    samples = [i for i in samples if len(i) > 0]\n",
        "    if state['method_of_var_analisis'] == 'ANOVA':\n",
        "        stat, pval = f_oneway(*samples)\n",
        "    elif state['method_of_var_analisis'] == 'KRUSKAL':\n",
        "        stat, pval = kruskal(*samples)\n",
        "\n",
        "    artefact = f'Так как признак распределен {'нормально' if state['is_normal_target'] else 'ненормально'}, то был проведен стат тест - {state['method_of_var_analisis']}. Его результаты: stat-{stat}, pval-{pval}'\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9z7444uza64"
      },
      "outputs": [],
      "source": [
        "def make_ling_var_regression(state: UserState) -> UserState:\n",
        "    data = state['data'].copy()\n",
        "    target = state['target']\n",
        "    group_var = state['grouping_variable']\n",
        "    group_vals = state['grouping_values']\n",
        "\n",
        "    if not is_numeric_dtype(data[group_var]):\n",
        "        data = data[data[group_var].isin(group_vals)]\n",
        "        data = pd.get_dummies(data, columns=[group_var], drop_first=True).astype('float')\n",
        "        features = [col for col in data.columns if col.startswith(f'{group_var}_')]\n",
        "        x = sm.add_constant(data[features])\n",
        "        y = data[target]\n",
        "    else:\n",
        "        y = (data[target] > group_vals[0]).astype(int)\n",
        "        x = sm.add_constant(data[group_var])\n",
        "\n",
        "    model = sm.OLS(y, x).fit()\n",
        "\n",
        "    artefact = f'Была построенна линейная регрессия, вот ее сводка: {str(model.summary())}'\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp1UYt7x7I6M"
      },
      "source": [
        "# Много групп и номинативный предиктор"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6dilj0_7JKb"
      },
      "outputs": [],
      "source": [
        "def make_logit_regression(state: UserState) -> UserState:\n",
        "    data = state['data'].copy()\n",
        "    target = state['target']\n",
        "    group_var = state['grouping_variable']\n",
        "    group_vals = state['grouping_values']\n",
        "\n",
        "    if not is_numeric_dtype(data[group_var]):\n",
        "        data = data[data[group_var].isin(group_vals)]\n",
        "        x_raw = pd.get_dummies(data[group_var], drop_first=True).astype(float)\n",
        "        x = sm.add_constant(x_raw)\n",
        "        y = pd.get_dummies(data[target], drop_first=True).iloc[:, 0].astype(float)\n",
        "    else:\n",
        "        y = (data[target] > group_vals[0]).astype(int)\n",
        "        x = sm.add_constant(data[group_var].astype(float))\n",
        "\n",
        "    if len(data[target].unique()) == 2:\n",
        "        print('Уже строю бинарную логистическую регрессию!')\n",
        "        model = sm.Logit(y, x).fit()\n",
        "    else:\n",
        "        model = sm.MNLogit(y, x).fit()\n",
        "\n",
        "    artefact = f'Была построенна логистическая регрессия, вот ее сводка: {str(model.summary())}'\n",
        "    return {'artefacts': state.get('artefacts', []) + [artefact]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsRszB_IlIxD"
      },
      "source": [
        "# Сводка по артефактам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6SlbCeKlRQi"
      },
      "outputs": [],
      "source": [
        "def summary_by_llm(state: UserState) -> UserState:\n",
        "    content = '\\n'.join(state['artefacts'])\n",
        "    #prompt = f'Ты помощник аналитика. Все исследование законченно, осталось только сделать КРАТКУЮ (100-200 слов) сводку. Также необходимо описать последованно этапы исследования с результатами.'\\\n",
        "    #'вот все артефакты исследования: {content}.'\\\n",
        "    #f'\\n\\nТак же можешь ознакомится с сообщениями во время исследования: {state['messages']}'\n",
        "\n",
        "    prompt = f'Было проведено исследование. Теперь необходимо сделать коротку сводку, где требуется как бы рассказать все этапы анализа данных!\\nВсе необходимые артефакты исследования здесь: {content}'\\\n",
        "    f'Для ознакомления, вот сообщения во время анализа данных: {state['messages']}'\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return {'summary': response.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc4Ot3osrT7h"
      },
      "source": [
        "## инициализация графа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23A5Fcw7e7IP"
      },
      "outputs": [],
      "source": [
        "graph = StateGraph(UserState)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCNtXsK7IbQ0",
        "outputId": "9f8291e7-d2ad-47f5-bc77-e16d7279424a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7876ab14c530>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.add_node('find_variables', find_variables)\n",
        "\n",
        "graph.add_node('is_correct_variables', is_correct_variables)\n",
        "\n",
        "graph.add_node('correcting_dtypes', process_dtypes_strictly)\n",
        "\n",
        "graph.add_node('cleaning_target', cleaning_target)\n",
        "\n",
        "graph.add_node('make_lin_reg', make_lin_reg)\n",
        "graph.add_node('make_corrs', make_corrs)\n",
        "\n",
        "graph.add_node('make_chi2', make_chi2)\n",
        "graph.add_node('make_fisher_exact', make_fisher_exact)\n",
        "\n",
        "graph.add_node('is_normal_distribution', is_normal_distribution)\n",
        "\n",
        "graph.add_node('make_samples', make_samples)\n",
        "\n",
        "graph.add_node('make_ttest', make_ttest)\n",
        "graph.add_node('make_welch_ttest', make_welch_ttest)\n",
        "graph.add_node('make_manayithui', make_manayithui)\n",
        "graph.add_node('make_standart_manayithui', make_standart_manayithui)\n",
        "graph.add_node('make_baket_manayithui', make_baket_manayithui)\n",
        "\n",
        "graph.add_node('choose_stat', choose_stat)\n",
        "graph.add_node('is_normal_distribution_for_var', is_normal_distribution_for_var)\n",
        "graph.add_node('make_var_analisis', make_var_analisis)\n",
        "graph.add_node('make_ling_var_regression', make_ling_var_regression)\n",
        "\n",
        "graph.add_node('make_logit_regression', make_logit_regression)\n",
        "\n",
        "graph.add_node('summary_by_llm', summary_by_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGn_alcAOdsP",
        "outputId": "5d8bc7a7-ea72-4afa-cf6f-6f0c8593e103"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7876ab14c530>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.add_edge(START, 'find_variables')\n",
        "\n",
        "graph.add_edge('find_variables', 'is_correct_variables')\n",
        "\n",
        "graph.add_conditional_edges('is_correct_variables'\n",
        "                            , lambda state: state['satisfied']\n",
        "                            , {\n",
        "                                'YES': 'correcting_dtypes'\n",
        "                                , 'NO': 'find_variables'\n",
        "                            })\n",
        "\n",
        "graph.add_edge('correcting_dtypes', 'cleaning_target')\n",
        "\n",
        "graph.add_conditional_edges('cleaning_target'\n",
        "                            , identifying_variables\n",
        "                            , {\n",
        "                                'ALL_NUMERIC': 'make_lin_reg'\n",
        "                                , 'ALL_NOMINATIV': 'make_chi2'\n",
        "                                , 'ONE_NUMERIC_AND_ONE_NOMINATIV': 'is_normal_distribution'\n",
        "                                , 'MANY_GROUPS_AND_NUMERIC_TARGET': 'is_normal_distribution_for_var'\n",
        "                                , 'MANY_GROUPS_AND_NOMINATIV_TARGET': 'make_logit_regression'\n",
        "                            })\n",
        "graph.add_edge('make_lin_reg', 'make_corrs')\n",
        "graph.add_edge('make_corrs', 'summary_by_llm')\n",
        "\n",
        "graph.add_edge('make_chi2', 'make_fisher_exact')\n",
        "graph.add_edge('make_fisher_exact', 'summary_by_llm')\n",
        "\n",
        "graph.add_edge('is_normal_distribution', 'make_samples')\n",
        "\n",
        "graph.add_conditional_edges('make_samples'\n",
        "                            , choose_stat_test\n",
        "                            , {\n",
        "                                'T_TEST': 'make_ttest'\n",
        "                                , 'WELCH_T_TEST': 'make_welch_ttest'\n",
        "                                , 'MANAYITHUI': 'make_manayithui'\n",
        "                            })\n",
        "graph.add_edge('make_ttest', 'summary_by_llm')\n",
        "graph.add_edge('make_welch_ttest', 'summary_by_llm')\n",
        "\n",
        "graph.add_conditional_edges('make_manayithui'\n",
        "                            , choose_method_to_make_manayithui\n",
        "                            , {\n",
        "                                'STANDART_MANAYITHUI': 'make_standart_manayithui'\n",
        "                                , 'BAKET_MANAYITHUI': 'make_baket_manayithui'\n",
        "                            })\n",
        "graph.add_edge('make_standart_manayithui', 'summary_by_llm')\n",
        "graph.add_edge('make_baket_manayithui', 'summary_by_llm')\n",
        "\n",
        "graph.add_edge('is_normal_distribution_for_var', 'choose_stat')\n",
        "graph.add_edge('choose_stat', 'make_var_analisis')\n",
        "graph.add_edge('make_var_analisis', 'make_ling_var_regression')\n",
        "graph.add_edge('make_ling_var_regression', 'summary_by_llm')\n",
        "\n",
        "graph.add_edge('make_logit_regression', 'summary_by_llm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLmGlgzPOhOH"
      },
      "outputs": [],
      "source": [
        "app = graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUdm_RUUOIKD"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\n",
        "    'eat': ['Free'] * 1000 + ['Middle'] * 1000 + ['Lux'] * 450,\n",
        "    'math_score': np.concatenate([\n",
        "        np.random.normal(66, 34, 1000),\n",
        "        np.random.normal(67, 34, 1000),\n",
        "        np.random.normal(83, 15, 450),\n",
        "    ])\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "5fdsRrCCOqWo",
        "outputId": "1ccaf819-605f-4484-965f-a070d3b25b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Выбран таргет: math_score, групповая переменная: eat, ее значения: Free Middle Lux\n",
            "Вас удовлетворяет выбор? Давай только без люкса\n",
            "Выбран таргет: math_score, групповая переменная: eat, ее значения: Free Middle\n",
            "Вас удовлетворяет выбор? ага\n"
          ]
        }
      ],
      "source": [
        "result = app.invoke({\n",
        "    'data': df\n",
        "    , 'messages': 'Привет! Если ученик питается платно, у него будет балл по математике выше?'\n",
        "}, {'recursion_limit': 1000})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GBsoeVIMPP1r",
        "outputId": "d088e320-0c06-4c67-82c6-55aca8fb90c8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'math_score'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.get('target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "enrkmQaTPaJE",
        "outputId": "bbff7ad0-0ecb-4c7b-c47b-4d68bbe7ac38"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eat'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.get('grouping_variable')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTUS21EKWAUF",
        "outputId": "7041bf07-524a-45d2-f2d6-35982967fc97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Free', 'Middle']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.get('grouping_values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bckgFfxYWzd",
        "outputId": "860d2b02-4936-455e-c53e-ce5cee173ef7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Привет! Если ученик питается платно, у него будет балл по математике выше?', additional_kwargs={}, response_metadata={}, id='9ea903c0-55fc-4c45-b844-2b159e9db51c'),\n",
              " SystemMessage(content='ОШИБКА: Пользователь недоволен. Исправь выбор. Пользователь хочет оставить target=math_score и group=eat, но хочет изменить переменную люкса на без люкса.', additional_kwargs={}, response_metadata={}, id='19d7adee-0ffe-4dcf-95be-28ea902ee00f')]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.get('messages')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v1d3uCOX20nr",
        "outputId": "aff656bf-5cf2-4709-a49f-d9b5cc01ae2d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'object'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.get('grouping_variable_dtype')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W0QGu-Tx26Uc",
        "outputId": "6f7e1d99-0a43-467e-891a-f39ee76a05d9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'float64'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.get('target_dtype')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B5CL8a-adYr",
        "outputId": "c7c50819-edc8-48f5-a729-74b10b33a3c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.get('is_normal_target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "f7EbD9uggIRE",
        "outputId": "4fb3883f-9a7f-43c5-8787-4bddcb44aa3a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**Сводка исследования**\\n\\nЦелью исследования было проанализировать связь между типом питания учеников и их баллами по математике. Для этого было проведено исследование, в котором были проанализированы данные о баллах по математике и типе питания учеников.\\n\\n**Этапы анализа данных**\\n\\n1. **Очистка данных**: Были удалены выбросы из данных, что позволило получить более точные результаты. Границы для удаления выбросов были установлены в диапазоне от -9,64 до 150,93. В результате было удалено 32 выброса.\\n2. **Визуальный анализ**: Были проведены визуальные анализы для двух групп: \"Free\" и \"Middle\". Результаты показали, что данные в обеих группах распределены нормально (тест Шапиро-Уилка: pval=0,0018 для группы \"Free\" и pval=0,0001 для группы \"Middle\").\\n3. **Проверка равенства дисперсий**: Был проведен тест Левенне, который показал, что выборочные дисперсии равны. Это позволило использовать t-тест для сравнения средних баллов по математике между двумя группами.\\n4. **t-тест**: Был проведен t-тест, который показал, что средние баллы по математике между двумя группами не различаются значимо (stat=-0,8305, pval=0,4064).\\n\\n**Выводы**\\n\\nРезультаты исследования показывают, что тип питания учеников не влияет на их баллы по математике. Это означает, что ученики, которые питаются платно, не имеют более высоких баллов по математике, чем ученики, которые питаются бесплатно.\\n\\n**Заключение**\\n\\nИсследование показало, что тип питания учеников не является значимым фактором, влияющим на их баллы по математике. Это может быть полезной информацией для педагогов и администраторов школ, которые хотят улучшить результаты учеников в математике.'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.get('summary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emd1bc1xSq5z",
        "outputId": "440d9ba7-e0cc-4903-9094-eef6e8bc35d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Сводка исследования**\n",
            "\n",
            "Целью исследования было проанализировать связь между типом питания учеников и их баллами по математике. Для этого было проведено исследование, в котором были проанализированы данные о баллах по математике и типе питания учеников.\n",
            "\n",
            "**Этапы анализа данных**\n",
            "\n",
            "1. **Очистка данных**: Были удалены выбросы из данных, что позволило получить более точные результаты. Границы для удаления выбросов были установлены в диапазоне от -9,64 до 150,93. В результате было удалено 32 выброса.\n",
            "2. **Визуальный анализ**: Были проведены визуальные анализы для двух групп: \"Free\" и \"Middle\". Результаты показали, что данные в обеих группах распределены нормально (тест Шапиро-Уилка: pval=0,0018 для группы \"Free\" и pval=0,0001 для группы \"Middle\").\n",
            "3. **Проверка равенства дисперсий**: Был проведен тест Левенне, который показал, что выборочные дисперсии равны. Это позволило использовать t-тест для сравнения средних баллов по математике между двумя группами.\n",
            "4. **t-тест**: Был проведен t-тест, который показал, что средние баллы по математике между двумя группами не различаются значимо (stat=-0,8305, pval=0,4064).\n",
            "\n",
            "**Выводы**\n",
            "\n",
            "Результаты исследования показывают, что тип питания учеников не влияет на их баллы по математике. Это означает, что ученики, которые питаются платно, не имеют более высоких баллов по математике, чем ученики, которые питаются бесплатно.\n",
            "\n",
            "**Заключение**\n",
            "\n",
            "Исследование показало, что тип питания учеников не является значимым фактором, влияющим на их баллы по математике. Это может быть полезной информацией для педагогов и администраторов школ, которые хотят улучшить результаты учеников в математике.\n"
          ]
        }
      ],
      "source": [
        "print(result.get('summary'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhkZLDXdAGCB"
      },
      "outputs": [],
      "source": [
        "def gen_png_graph(app_obj, name_photo: str = 'graph.png') -> None:\n",
        "    with open(name_photo, 'wb') as f:\n",
        "        f.write(app_obj.get_graph().draw_mermaid_png())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6gHQkr8AGPg"
      },
      "outputs": [],
      "source": [
        "gen_png_graph(app)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qApUKgsdAHQs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Nvfm4rmPFRwf",
        "lSSinVlbFZvM",
        "5Ww4hzi3F1y1",
        "AchGz4SyrLAU",
        "azZAUgmiZlt3",
        "oB1nSxWRevKn",
        "jSE1qbi9qA42",
        "6Ni9XOUitVV3",
        "n14-ojy1MaNw",
        "Fp1UYt7x7I6M"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
